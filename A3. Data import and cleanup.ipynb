{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Data import and cleanup\n",
    "\n",
    "In my experience, around 75% of the time you spend working with data will be fighting to import it and clean it up. For the most part this is just general-purpose programming, but there are a few library routines that will save you from reinventing the wheel.\n",
    "\n",
    "* [A.0 Reading a simple text file](#readfile)\n",
    "* [A.1 Reading from a URL](#readurl)\n",
    "* [A.2 Parsing a log file with regular expressions](#regexp)\n",
    "* [A.3 Reading JSON from a web service](#json)\n",
    "* [A.4 Scraping a website with XPath](#xpath)\n",
    "* [A.5 Reading from an SQL database](#sql)\n",
    "\n",
    "Treat this section as a collection of recipes and pointers to useful library routines. If you find yourself needing them, you should read the recipe, try it out, then look online for more information about the library functions it suggests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see plenty of plots, but this notebook is not a tutorial about plotting. For that, have a look at the [matplotlib gallery](https://matplotlib.org/gallery.html), and find recipes for how to achieve the effects you want.\n",
    "\n",
    "Scientific computing is all about the data: how to handle it, how to plot it, how to reason about it. We could program \n",
    "all of our data handling directly in Python or some other low-level language &mdash; but it's better to use high-level expressive libraries, so we spend time thinking about what the data means and not about how it's stored or indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " For extreme quirks you may need to use the raw Python [`csv.reader`](https://docs.python.org/3/library/csv.html#csv.reader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "At the top of almost every piece of scientific computing work, we'll import these standard modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules, and give them short aliases so we can write e.g. np.foo rather than numpy.foo\n",
    "import math, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import pandas\n",
    "# The next line is a piece of magic, to let plots appear in our Jupyter notebooks\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.0 Reading from a file <a name=\"readfile\"></a>\n",
    "When your data is a very simple comma-separated value (CSV) file then it's very easy to import. A CSV file looks like this: a header line, then one line per row of the data frame, values separated by commas.\n",
    "```\n",
    "\"Sepal.Length\",\"Sepal.Width\",\"Petal.Length\",\"Petal.Width\",\"Species\"\n",
    "5.1,3.5,1.4,0.2,\"setosa\"\n",
    "4.9,3,1.4,0.2,\"setosa\"\n",
    "4.7,3.2,1.3,0.2,\"setosa\"\n",
    "4.6,3.1,1.5,0.2,\"setosa\"\n",
    "5,3.6,1.4,0.2,\"setosa\"\n",
    "```\n",
    "Here is code to import a simple CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/iris.csv') as f:\n",
    "    df = pandas.read_csv(f)                   # this returns a pandas.DataFrame\n",
    "    df = {col:df[col].values for col in df}   # (optional) convert it to dict of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `with` syntax is useful for files and similar objects which need to be closed/released after we've finished with them, to avoid memory leaks. It's equivalent to calling `f=open('data/iris.csv')`, then running the other commands, then calling `f.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your file is nearly a CSV but has some quirks such as comments or a missing header row, experiment with the options in [`pandas.read_csv`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) or [`pandas.read_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html). For extreme quirks you may need to use the raw Python [`csv.reader`](https://docs.python.org/3/library/csv.html#csv.reader)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1 Reading from a URL<a name=\"readurl\"></a>\n",
    "You can use the same `pandas.read_csv` function to read any file-like thing, such as a files retrieved over the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request  # standard library for web requests\n",
    "my_url = \"https://raw.githubusercontent.com/damonjw/scicomp/master/data/iris.csv\"\n",
    "with urllib.request.urlopen(my_url) as f:\n",
    "    df = pandas.read_csv(f)\n",
    "    df = {col:df[col].values for col in df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also read from a string as if it were a file using [`io.StringIO`](https://docs.python.org/3/library/io.html#io.StringIO). This is sometimes handy for debugging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2 Parsing a text log file<span id=\"regexp\"></span>\n",
    "A typical line from a web server log might look like this\n",
    "```\n",
    "207.46.13.169 - - [27/Aug/2017:06:52:11 +0000] \"GET /marcus/essay/st&h2.html HTTP/1.1\" 200 3881 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\"\n",
    "```\n",
    "where (according to the [Apache web server documentation](https://httpd.apache.org/docs/2.4/logs.html#combined)) the pieces are\n",
    "\n",
    "* **`207.46.13.169`** \n",
    "The IP address that made the request\n",
    "* **`-`** \n",
    "The identity of the client; `-` means not available\n",
    "* **`-`**\n",
    "The userid of the logged-in user who made the request; `-` means not available\n",
    "* **`[27/Aug/2017:06:52:11 +0000]`**\n",
    "The time the request was received\n",
    "* **`\"GET /marcus/essay/st&h2.html HTTP/1.1\"`**\n",
    "The type of request, what was requested, and the connection type\n",
    "* **`200`**\n",
    "The [http status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) (200 means OK)\n",
    "* **`3881`**\n",
    "The size of the object returned to the client, in bytes\n",
    "* **`-`**\n",
    "The referrer URL; `-` means not available\n",
    "* **`\"Mozilla/5.0 (...)\"`**\n",
    "The browser type. The substring `bingbot` here tells us that the request comes from Microsoft Bing's web crawler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract these pieces from a single line of the log file, the best tool is [regular expressions](https://docs.python.org/3.4/library/re.html), a mini-language for string matching that is common across many programming languages. The syntax is terse and takes a lot of practice. I like to start with a small string pattern and incrementally build it up, testing as I go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re    # standard Python module for regular expressions\n",
    "s = \"\"\"\n",
    "207.46.13.169 - - [27/Aug/2017:06:52:11 +0000] \"GET /marcus/essay/st&h2.html HTTP/1.1\" \n",
    "200 3881 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X)\n",
    "AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53\n",
    "(compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\"\n",
    "\"\"\"\n",
    "\n",
    "# First attempt: match the first two items in the log line.\n",
    "# If my pattern is right, re.match returns an object.\n",
    "# If my pattern is wrong, re.match returns None.\n",
    "pattern_test = r'\\s*(\\S+)\\s*(\\S+)'\n",
    "re.match(pattern_test, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the full pattern I built up to. Python lets us add verbose comments\n",
    "# to the pattern, which is handy for remembering what your code does when you look\n",
    "# at it the next morning.\n",
    "pattern = r'''(?x)  #   flag saying that this pattern has comments\n",
    "\\s*                 #   any whitespace at the start of the string\n",
    "(?P<ip>\\S+)         # one or more non-space characters: the IP address\n",
    "\\s+                 #   one or more spaces\n",
    "(?P<client>\\S+)     # the client identity\n",
    "\\s+\n",
    "(?P<user>\\S+)       # the userid\n",
    "\\s+\n",
    "\\[(?P<t>[^\\]]*)\\]   # [, then any number of not-] characters, then ]: the timestamp\n",
    "\\s+\n",
    "\"(?P<req>[^\"]*)\"    # \", then any number of not-\" characters, then \": the request string\n",
    "\\s+\n",
    "(?P<status>\\d+)     # one or more numerical digits: the http status code\n",
    "\\s+\n",
    "(?P<size>\\d+)       # one or more numerical digits: the size\n",
    "\\s+\n",
    "\"(?P<ref>[^\"]*)\"    # the referrer URL\n",
    "\\s+\n",
    "\"(?P<ua>[^\"]*)\"     # the user agent i.e. browser type\n",
    "'''\n",
    "m = re.match(pattern, s)\n",
    "m.groupdict()       # returns a dictionary of all the named sub-patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Why not use `s.split(' ')`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we extract the fields from a full log file? The standard Python code is\n",
    "```\n",
    "with open(myfile) as f:\n",
    "    for line in f:\n",
    "        m = re.match(pattern, line)\n",
    "        # store the fields from m.groups() or m.groupdict() somewhere appropriate\n",
    "```\n",
    "Alternatively, `numpy` has a handy shortcut for reading in an entire file and splitting it via a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the file into an array, one row per line, one column per field in the pattern\n",
    "df = np.fromregex('data/access_short.log', pattern, dtype=np.unicode_)\n",
    "\n",
    "# Make a dictionary out of the columns, according to the named fields in the pattern\n",
    "df = {k: df[:,v-1] for k,v in re.compile(pattern).groupindex.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3 Importing from a web data service<a name=\"json\"></a>\n",
    "More and more forward-thinking companies and government services make data available by simple web requests. Here is an example, importing river levels from the UK's [real-time flood monitoring API](http://environment.data.gov.uk/flood-monitoring/doc/reference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to import the Python module for making web requests. When I'm developing data code I like to build it up in small steps, which means lots of repeated requests, so I also like to use another Python module which caches responses. This means I don't hammer the service unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, requests_cache\n",
    "requests_cache.install_cache('floodsystem', backend='memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [flood monitoring API documentation](http://environment.data.gov.uk/flood-monitoring/doc/reference) tells us the URL for fetching a list of all stations, `{root}/id/stations`. Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the request. This should print out <Response [200]>, meaning successfully retrieved\n",
    "stations_resp = requests.get('http://environment.data.gov.uk/flood-monitoring/id/stations')\n",
    "stations_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the body of the response. It's likely to be very long, so we'll only print out the first 300 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations_resp.text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like [JSON](https://en.wikipedia.org/wiki/JSON), \"JavaScript Object Notation\", a common format for web data services. It's easy to turn it it into Python dictionaries and lists, and to explore what it contains. (Alternatively, just read the web service documentation, if you trust it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = stations_resp.json()\n",
    "print(x.__class__)                        # The top-level response is a dictionary\n",
    "print(x.keys())                           # What are its keys? 'items' sounds promising.\n",
    "print(x['items'].__class__)               # 'items' is a list,\n",
    "print(len(x['items']))                    # with 4363 elements.\n",
    "\n",
    "stations = stations_resp.json()['items']  # This is what we're really after\n",
    "stations[0]                               # Look at a single sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from web services is often patchy and inconsistent, so your code for processing it should be full of error handling. For example,\n",
    "```\n",
    "rivers = [s['riverName'] for s in stations]\n",
    "```\n",
    "will fail because `riverName` isn't present for all stations. You might use one of these instead:\n",
    "```\n",
    "rivers = [s.get('riverName', None) for s in stations]\n",
    "rivers = [s['riverName'] for s in stations if 'riverName' in s]\n",
    "```\n",
    "Here's how we might turn the data into a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = {\n",
    "    'label': np.array([s['label'] for s in stations]),\n",
    "    'riverName': np.array([s.get('riverName',None) for s in stations]),\n",
    "    'town': np.array([s.get('town',None) for s in stations])\n",
    "}\n",
    "\n",
    "# Each station has zero or more measures. Pick out the first 'water level' measure at each.\n",
    "# The field measure['@id'] is the url to use to get water level readings.\n",
    "measures = [[m for m in s.get('measures',{}) if m['parameter']=='level'] for s in stations]\n",
    "df['url'] = np.array([ms[0]['@id'] if len(ms)>0 else None for ms in measures])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4 Scraping a website<a name=\"xpath\"></a>\n",
    "There are fascinating stories to be discovered from public data, and sometimes you have to work to scrape it from web pages. Here's [an acount](https://onlinejournalismblog.com/2016/11/29/how-the-bbc-england-data-unit-scraped-airport-noise-complaints/) by a BBC data journalist. We'll work with a very simple example: extracting results of the Oxford / Cambridge boat race from the [Wikipedia table](https://en.wikipedia.org/wiki/List_of_The_Boat_Race_results#Main_race).\n",
    "\n",
    "I recommend using [XPath queries](https://www.w3.org/TR/xpath20/) from the [`lxml`](http://lxml.de/) module.\n",
    "XPath is a powerful mini-language for extracting data from hierarchical documents, with wide support across many programming languages &mdash; think of it as regular expressions but for html rather than plain text. If you want to scrape websites then it's worth finding a tutorial and learning XPath. For this course, we'll just see how to use XPath in Python.\n",
    "\n",
    "The first step is to install `lxml`, which is not included with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip3 install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fetch the web page and parse the contents. Most web pages are badly-formatted html (sections not properly closed, etc.), and `lxml.html.fromstring` makes a reasonable attempt to make sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "boatrace_url = 'https://en.wikipedia.org/wiki/List_of_The_Boat_Race_results'\n",
    "resp = requests.get(boatrace_url)\n",
    "doc = lxml.html.fromstring(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us `doc`, the root `<html>` element, which we can inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(doc.tag)               # the type of element\n",
    "print(len(doc))              # the number of children\n",
    "print([n.tag for n in doc])  # tags of its children, <head> and <body>\n",
    "print(doc.attrib)            # get the attributes, e.g. <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
    "print(doc.text, doc.tail)    # any text directly under in this element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to pull out a particular element from the document, namely the table with boat race results, so we need to work out how to refer to it in XPath. The Chrome webbrowser has a handy tool to help with this. Go to the page you're interested in, and click on View | Developer | Developer Tools. Click on the element-selector button at the top left:\n",
    "<img src=\"fig/xpath1.png\" alt=\"use 'select element' mode\">\n",
    "Go back to the web page, and click on a piece close to what you want to select. I clicked on the top left cell of the table:\n",
    "<img src=\"fig/xpath2.png\" alt=\"click roughly where you want\" style=\"height:8em\">\n",
    "Go back to the developer tools window, and navigate to the exact element you want. Here, we want the table. Right-click and choose Copy | Copy XPath. \n",
    "<img src=\"fig/xpath3.png\" alt=\"copy the XPath of the element you want\" style=\"height:8em\">\n",
    "It gave me the XPath location `\"//*[@id=\"mw-content-text\"]/div/table[2]\"`. Now we can extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pick out the table element.\n",
    "# (XPath queries return lists, but I only want one item, hence the [0].)\n",
    "table = doc.xpath('//*[@id=\"mw-content-text\"]/div/table[2]')[0]\n",
    "\n",
    "# Get a list of all rows i.e. <tr> elements inside the table.\n",
    "# Print one, to check things look OK.\n",
    "rows = table.xpath('.//tr')\n",
    "print(lxml.etree.tostring(rows[1], encoding='unicode'))\n",
    "\n",
    "# Extract the timestamp and winner columns.\n",
    "# The timestamp is in the second child, in a <span> element with class \"sortkey\".\n",
    "# The winner is in the third child.\n",
    "df = {'t': [row[1].xpath('.//span[contains(@class, \"sortkey\")]')[0].text for row in rows[1:]],\n",
    "      'winner': [row[2].text for row in rows[1:]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should consider the ethics of your web scraping. Here are some thoughts:\n",
    "from [Sophie Chou at the MIT Media Lab](http://www.storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web/), and the [data journalist N&auml;el Shiab](https://gijn.org/2015/08/12/on-the-ethics-of-web-scraping-and-data-journalism/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.5 Reading from an SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "import gmplot\n",
    "import tempfile\n",
    "import webbrowser\n",
    "\n",
    "with open('/Users/djw/djw1005/etc/credentials.json') as f:\n",
    "    creds = json.load(f)['db']['local']\n",
    "conn = psycopg2.connect(user=creds['user'], dbname=creds['dbname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uri</th>\n",
       "      <th>catchment</th>\n",
       "      <th>river</th>\n",
       "      <th>town</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>index</th>\n",
       "      <th>measure_id</th>\n",
       "      <th>uri</th>\n",
       "      <th>station_uri</th>\n",
       "      <th>qualifier</th>\n",
       "      <th>parameter</th>\n",
       "      <th>period</th>\n",
       "      <th>unit</th>\n",
       "      <th>valuetype</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>345</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Cam and Ely Ouse (Including South Level)</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Great Chesterford</td>\n",
       "      <td>52.061730</td>\n",
       "      <td>0.194279</td>\n",
       "      <td>397</td>\n",
       "      <td>398</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>m</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>800</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Parrett, Brue and West Somerset Streams</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Weston Bampfylde</td>\n",
       "      <td>51.023159</td>\n",
       "      <td>-2.565568</td>\n",
       "      <td>918</td>\n",
       "      <td>919</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>m</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1272</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Cam and Ely Ouse (Including South Level)</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Milton</td>\n",
       "      <td>52.236542</td>\n",
       "      <td>0.176925</td>\n",
       "      <td>1454</td>\n",
       "      <td>1455</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>mASD</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1435</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Severn Vale</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Cam</td>\n",
       "      <td>51.699985</td>\n",
       "      <td>-2.360238</td>\n",
       "      <td>1635</td>\n",
       "      <td>1636</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>mASD</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1492</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Cam and Ely Ouse (Including South Level)</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>52.212835</td>\n",
       "      <td>0.120872</td>\n",
       "      <td>1701</td>\n",
       "      <td>1702</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>mASD</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1509</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Cam and Ely Ouse (Including South Level)</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Great Shelford</td>\n",
       "      <td>52.134462</td>\n",
       "      <td>0.141784</td>\n",
       "      <td>1723</td>\n",
       "      <td>1724</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>m</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1568</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Cam and Ely Ouse (Including South Level)</td>\n",
       "      <td>Bin Brook</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>52.197227</td>\n",
       "      <td>0.087527</td>\n",
       "      <td>1791</td>\n",
       "      <td>1792</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>mASD</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1635</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Severn Vale</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>51.730432</td>\n",
       "      <td>-2.362218</td>\n",
       "      <td>1864</td>\n",
       "      <td>1865</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>mASD</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.141</td>\n",
       "      <td>1.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1668</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Cam and Ely Ouse (Including South Level)</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Waterbeach</td>\n",
       "      <td>52.268775</td>\n",
       "      <td>0.208550</td>\n",
       "      <td>1901</td>\n",
       "      <td>1902</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>mASD</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                uri  \\\n",
       "0    345  http://environment.data.gov.uk/flood-monitorin...   \n",
       "1    800  http://environment.data.gov.uk/flood-monitorin...   \n",
       "2   1272  http://environment.data.gov.uk/flood-monitorin...   \n",
       "3   1435  http://environment.data.gov.uk/flood-monitorin...   \n",
       "4   1492  http://environment.data.gov.uk/flood-monitorin...   \n",
       "5   1509  http://environment.data.gov.uk/flood-monitorin...   \n",
       "6   1568  http://environment.data.gov.uk/flood-monitorin...   \n",
       "7   1635  http://environment.data.gov.uk/flood-monitorin...   \n",
       "8   1668  http://environment.data.gov.uk/flood-monitorin...   \n",
       "\n",
       "                                  catchment      river               town  \\\n",
       "0  Cam and Ely Ouse (Including South Level)  River Cam  Great Chesterford   \n",
       "1   Parrett, Brue and West Somerset Streams  River Cam   Weston Bampfylde   \n",
       "2  Cam and Ely Ouse (Including South Level)  River Cam             Milton   \n",
       "3                               Severn Vale  River Cam                Cam   \n",
       "4  Cam and Ely Ouse (Including South Level)  River Cam          Cambridge   \n",
       "5  Cam and Ely Ouse (Including South Level)  River Cam     Great Shelford   \n",
       "6  Cam and Ely Ouse (Including South Level)  Bin Brook          Cambridge   \n",
       "7                               Severn Vale  River Cam          Cambridge   \n",
       "8  Cam and Ely Ouse (Including South Level)  River Cam         Waterbeach   \n",
       "\n",
       "         lat       lng  index  measure_id  \\\n",
       "0  52.061730  0.194279    397         398   \n",
       "1  51.023159 -2.565568    918         919   \n",
       "2  52.236542  0.176925   1454        1455   \n",
       "3  51.699985 -2.360238   1635        1636   \n",
       "4  52.212835  0.120872   1701        1702   \n",
       "5  52.134462  0.141784   1723        1724   \n",
       "6  52.197227  0.087527   1791        1792   \n",
       "7  51.730432 -2.362218   1864        1865   \n",
       "8  52.268775  0.208550   1901        1902   \n",
       "\n",
       "                                                 uri  \\\n",
       "0  http://environment.data.gov.uk/flood-monitorin...   \n",
       "1  http://environment.data.gov.uk/flood-monitorin...   \n",
       "2  http://environment.data.gov.uk/flood-monitorin...   \n",
       "3  http://environment.data.gov.uk/flood-monitorin...   \n",
       "4  http://environment.data.gov.uk/flood-monitorin...   \n",
       "5  http://environment.data.gov.uk/flood-monitorin...   \n",
       "6  http://environment.data.gov.uk/flood-monitorin...   \n",
       "7  http://environment.data.gov.uk/flood-monitorin...   \n",
       "8  http://environment.data.gov.uk/flood-monitorin...   \n",
       "\n",
       "                                         station_uri qualifier    parameter  \\\n",
       "0  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "1  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "2  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "3  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "4  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "5  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "6  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "7  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "8  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "\n",
       "   period  unit      valuetype    low   high  \n",
       "0   900.0     m  instantaneous  0.109  0.333  \n",
       "1   900.0     m  instantaneous  0.026  0.600  \n",
       "2   900.0  mASD  instantaneous  0.218  0.294  \n",
       "3   900.0  mASD  instantaneous  0.578  0.782  \n",
       "4   900.0  mASD  instantaneous  0.637  0.740  \n",
       "5   900.0     m  instantaneous  0.127  0.395  \n",
       "6   900.0  mASD  instantaneous  0.057  0.368  \n",
       "7   900.0  mASD  instantaneous  0.141  1.250  \n",
       "8   900.0  mASD  instantaneous  0.240  0.316  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = '''\n",
    "SELECT *\n",
    "FROM flood_stations AS s JOIN flood_measures AS m ON (m.station_uri = s.uri) \n",
    "WHERE river = 'River Cam' OR town = 'Cambridge'\n",
    "'''\n",
    "df = pandas.read_sql(cmd, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmap = gmplot.GoogleMapPlotter(52.212, 0.1208, 16)\n",
    "gmap.scatter(df['lat'], df['lng'], 'red', size=500, marker=False)\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix='.html') as f:\n",
    "    gmap.draw(f.name)\n",
    "    webbrowser.open('file://' + f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd = '''\n",
    "SELECT s.label, s.id, lat, lng, town, river, measure_id, qualifier, parameter, period, low, high, valuetype, t, value\n",
    "FROM flood_stations AS s\n",
    "    JOIN flood_measures AS m ON (m.station_uri = s.uri)\n",
    "    JOIN flood_readings AS r USING (measure_id)\n",
    "WHERE s.river = 'River Cam' or s.town = 'Cambridge'\n",
    "'''\n",
    "df = pandas.read_sql(cmd, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['value'] = df['value'].values.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('data/flood.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.6 Wrangling with time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
